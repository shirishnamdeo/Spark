import org.apache.spark.SparkConf

SparkConf sparkConf = new SparkConf()
					.setAppName("Print Elements of RDD")
        	        .setMaster("local[2]")
					.set("spark.executor.memory","2g");


appName parameter is a name for your application to show on the cluster UI

master is a Spark, Mesos or YARN cluster URL, or a special “local” string to run in local mode.
	In practice, when running on a cluster, you will not want to hardcode master in the program, but rather launch the application with spark-submit and receive it there. 
	However, for local testing and unit tests, you can pass “local” to run Spark in-process.	

