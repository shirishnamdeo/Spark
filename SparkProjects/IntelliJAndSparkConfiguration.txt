https://kaizen.itversity.com/setup-development-environment-intellij-and-scala-big-data-hadoop-and-spark/


Typically programming will be done with IDEs such as IntelliJ
IDEs are typically integrated with other tools such as git which is code versioning tool. Tools like git facilitate team development.
sbt is build tool for Scala. Once applications are developed using IDE, they are typically built using tools like sbt


-- https://www.youtube.com/watch?v=Z7JXDg5jltQ
-- Make usre Java with JDK is installed
-- Install Scala with IDE (prefered IntelliJ)
   Scala is JVM based programming language, so once the programs is been developed, it has to be compiled into ByteCode and it has be archived (jar file is to be created)
-- To compile the Scala based code and generate the Archive, we use SBT (Simple built tool)
-- To develop spark based applications especially on Windoes, we need to Setup winutils.exe 




-- JDK Setup:- https://www.youtube.com/watch?v=uQV7Q59RM-g
-- Scala and IntellIj:- https://www.youtube.com/watch?v=MY5PSSYZplg
   Once donwloaded, you can start by Create New Project, and you should see SCALA there.
   Click on Left-Side Scala option and make sure sbt is visible now.
-- Use Scala -> SBT (and Not Java -> Scala)


-- Demo Project:- https://www.youtube.com/watch?v=b4Tc2QYHO4g


-- It might not always be correct to use the default version of SBT and Scala provided by default IntelliJ.
-- Make sure they are compatible with each other. Make Sure Spark verions is compatible with SCALA

-- ON spark-shell run
scala> util.Properties.versionString
-- res0: String = version 2.11.12
-- Select the given version of SCALA


-- Other ways
scala> util.Properties.versionNumberString
res1: String = 2.11.12

scala> util.Properties.versionMsg
res2: String = Scala library version 2.11.12 -- Copyright 2002-2017, LAMP/EPFL


-- With Scala 2.11.12 use SBT -> 0.13.18 (Based on scala verison use the SBT version)


-- scalaVersion in ThisBuild := "2.10.0"
-- https://www.scala-sbt.org/release/docs/Multi-Project.html


After creating the project IntelliJ will download some files. I will take a while to complete the download. Once that is done we can start with Scala application.
-- sbt: dump project structire from sbt --> Taking too much time (+40 min and still going on)
-- I used the latest verison of SBT, and now IntelliJ downloaded in around few minutes.
-- once downloaded, the sbt.build will have no errors.



In project structure, the version of the libs to be used and downloaded shoudl be described in 'built.sbt'
Import project in the top will take care of downloading and importing the binaries for us.


-- We need to create a scala class. Go to src/main/scala. Right click on Scala -> New -> Scala class.
-- You might do not see the Scala Class in the list and this is due to Scala Framework being not been added to our project.
-- Right click on your project, "Add Framework support" and choose Scala framework if possible.
-- Else one reason coudl be that the IntellIj has not marked the src folder as Srouce folder. Do it as below:
   Project Structure -> Modles. Mark src folder as Srouce -> Apply



https://www.youtube.com/watch?v=fdiRcraN8hU
Setup sbt (build tool) and run application [Build a Jar File]

Why sbt?
    To build scala based applications to jar file
    Validate jar file to make sure program is running fine

To package with STB -- You need to download and Install the SBT for this. But I think IntellIj can itself build the JAR file for us using SBT
-- sbt package (to build the JAR FILE)
-- sbt run (to try running the JAR file)




Add Spark dependencies to the application:
Tutorial: https://www.youtube.com/watch?v=rdVyoeh1VTc

Update build.sbt by adding
libraryDependencies += "org.apache.spark" %% "spark-core" % "2.3.0"
-- Enable auto-import or click on refresh on type right corner
-- It will take some time to download dependencies based on your internet speed

-- Note the Spark Version (can get it from the spark-shell)
scala> spark.version
res0: String = 2.4.0

scala> sc.version
res1: String = 2.4.0




Setup WinUtils to get HDFS APIs working
https://www.youtube.com/watch?v=Dym79mWN_rk
Why to install winutils?
    In the process of building data processing applications using Spark, we need to read data from files
    Spark uses HDFS API to read files from several file systems like HDFS, s3, local etc
    For HDFS APIs to work on Windows, we need to have WinUtils





Setup Data sets
You need to have data sets setup for your practice.
-- Go to our GitHub data repository: (no data found)





Develop first spark application
https://www.youtube.com/watch?v=SXFcQj_nsos


Go to src/main/scala
    Right click and click on New -> Package
    Give the package name as package_name
    Right click on package_name and click on New -> Scala Class
    Name: ObjectFileName
    Type: Object