
References:




-----------------------------------------------------------------------------------------------------------------------------------------------------

Spark Streaming Overview
Discretized Streams (DStreams)



-----------------------------------------------------------------------------------------------------------------------------------------------------

Spark Streaming Overview:

Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams.

Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with 
high-level functions like map, reduce, join and window. 

Finally, processed data can be pushed out to filesystems, databases, and live dashboards. 
In fact, you can apply Sparkâ€™s MACHINE LEARNING and GRAPH PROCESSING ALGORITHMS on data streams.


Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by 
the Spark engine to generate the final stream of results in batches.


Spark Streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data. 
DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level operations on other DStreams. 
Internally, a DStream is represented as a sequence of RDDs.



Example (Reading from Socket):

StreamingContext is the main entry point for all streaming functionality. 
We create a local StreamingContext with two execution threads, and a batch interval of 1 second.
-- The master requires 2 cores to prevent a starvation scenario.


import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._    --  not necessary since Spark 1.3

val conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount")
--  val ssc = new StreamingContext(conf, Seconds(1))  --   In Spark-Shell, the SparkContext already exists thus getting an error.
--  Note that StreamingContext internally creates a SparkContext which can be accessed as ssc.sparkContext. [That's why in shell it is giving an error!!]

val ssc = new StreamingContext(sc, Seconds(1))       --   So using sc instead of conf
--  ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@1988e095


val lines = ssc.socketTextStream("localhost", 9999)
--  Create a DStream that represents streaming data from a TCP source, specified as hostname and port.
--  lines: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@63fcbf75
--  Each record in this DStream is a line of text.



val words = lines.flatMap(_.split(" "))
--  words: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.FlatMappedDStream@47db20e0
--  flatMap is a one-to-many DStream operation that creates a new DStream by generating multiple new records from each record in the source DStream. 
--  In this case, each line will be split into multiple words and the stream of words is represented as the words DStream.



import org.apache.spark.streaming.StreamingContext._    -- not necessary since Spark 1.3


val pairs = words.map(word => (word, 1))
val wordCounts = pairs.reduceByKey(_ + _)
wordCounts.print()



** Note that when these lines are executed, Spark Streaming only sets up the computation it will perform when it is started, and no real processing has started yet.


To start the processing after all the transformations have been setup, we finally call

-- Make sure to open a TCP port for spark to listen to before starting the streaming application.
-- nc -l -p 9999

ssc.start()             // Start the computation
ssc.awaitTermination()  // Wait for the computation to terminate


--  The processing can be manually stopped using streamingContext.stop()

--  Once a context has been started, no new streaming computations can be set up or added to it.
--  Once a context has been stopped, it cannot be restarted. Why???
--  Only one StreamingContext can be active in a JVM at the same time.
--  stop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set the optional parameter of stop() called stopSparkContext to false.
--  A SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped (without stopping the SparkContext) before the next StreamingContext is created.







-----------------------------------------------------------------------------------------------------------------------------------------------------

Discretized Streams (DStreams)


Discretized Stream or DStream is the basic abstraction provided by Spark Streaming. 
It represents a continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream.

Internally, a DStream is represented by a continuous series of RDDs.
Each RDD in a DStream contains data from a certain interval.

Any operation applied on a DStream translates to operations on the underlying RDDs
For example, in the earlier example of converting a stream of lines to words, the flatMap operation is applied on each RDD in the lines DStream to generate the RDDs of the words DStream.

These underlying RDD transformations are computed by the Spark engine




Input DStreams and Receivers:



