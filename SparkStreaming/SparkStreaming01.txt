
References:




-----------------------------------------------------------------------------------------------------------------------------------------------------

Spark Streaming Overview
Discretized Streams (DStreams)



-----------------------------------------------------------------------------------------------------------------------------------------------------

Spark Streaming Overview:

Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams.

Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with 
high-level functions like map, reduce, join and window. 

Finally, processed data can be pushed out to filesystems, databases, and live dashboards. 
In fact, you can apply Spark’s MACHINE LEARNING and GRAPH PROCESSING ALGORITHMS on data streams.


Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by 
the Spark engine to generate the final stream of results in batches.


Spark Streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data. 
DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level operations on other DStreams. 
Internally, a DStream is represented as a sequence of RDDs.



Example (Reading from Socket):

StreamingContext is the main entry point for all streaming functionality. 
We create a local StreamingContext with two execution threads, and a batch interval of 1 second.
-- The master requires 2 cores to prevent a starvation scenario.


import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._    --  not necessary since Spark 1.3

val conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount")
--  val ssc = new StreamingContext(conf, Seconds(1))  --   In Spark-Shell, the SparkContext already exists thus getting an error.
--  Note that StreamingContext internally creates a SparkContext which can be accessed as ssc.sparkContext. [That's why in shell it is giving an error!!]

val ssc = new StreamingContext(sc, Seconds(1))       --   So using sc instead of conf
--  ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@1988e095


val lines = ssc.socketTextStream("localhost", 9999)
--  Create a DStream that represents streaming data from a TCP source, specified as hostname and port.
--  lines: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@63fcbf75
--  Each record in this DStream is a line of text.



val words = lines.flatMap(_.split(" "))
--  words: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.FlatMappedDStream@47db20e0
--  flatMap is a one-to-many DStream operation that creates a new DStream by generating multiple new records from each record in the source DStream. 
--  In this case, each line will be split into multiple words and the stream of words is represented as the words DStream.



import org.apache.spark.streaming.StreamingContext._    -- not necessary since Spark 1.3


val pairs = words.map(word => (word, 1))
val wordCounts = pairs.reduceByKey(_ + _)
wordCounts.print()



** Note that when these lines are executed, Spark Streaming only sets up the computation it will perform when it is started, and no real processing has started yet.


To start the processing after all the transformations have been setup, we finally call

-- Make sure to open a TCP port for spark to listen to before starting the streaming application.
-- nc -l -p 9999

ssc.start()             // Start the computation
ssc.awaitTermination()  // Wait for the computation to terminate


--  The processing can be manually stopped using streamingContext.stop()

--  Once a context has been started, no new streaming computations can be set up or added to it.
--  Once a context has been stopped, it cannot be restarted. Why???
--  Only one StreamingContext can be active in a JVM at the same time.
--  stop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set the optional parameter of stop() called stopSparkContext to false.
--  A SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped (without stopping the SparkContext) before the next StreamingContext is created.







-----------------------------------------------------------------------------------------------------------------------------------------------------

Discretized Streams (DStreams)


Discretized Stream or DStream is the basic abstraction provided by Spark Streaming. 
It represents a continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream.

Internally, a DStream is represented by a continuous series of RDDs.
Each RDD in a DStream contains data from a certain interval.

Any operation applied on a DStream translates to operations on the underlying RDDs
For example, in the earlier example of converting a stream of lines to words, the flatMap operation is applied on each RDD in the lines DStream to generate the RDDs of the words DStream.

These underlying RDD transformations are computed by the Spark engine




Input DStreams and Receivers:

Input DStreams are DStreams representing the stream of input data received from streaming sources.
Every input DStream (except file stream) is associated with a Receiver object which receives the data from a source and stores it in Spark’s memory for processing.


Spark Streaming provides two categories of built-in streaming sources.
	Basic sources: 
		Sources directly available in the StreamingContext API. Examples: file systems, and socket connections.
	
	Advanced sources: 
		Sources like Kafka, Flume, Kinesis, etc. are available through extra utility classes. 
		These require linking against extra dependencies as discussed in the linking section.



Note that, if you want to receive multiple streams of data in parallel in your streaming application, you can create multiple input DStreams (discussed further in the Performance Tuning section). 
This will create multiple receivers which will simultaneously receive multiple data streams. 
But note that a Spark worker/executor is a long-running task, hence it occupies one of the cores allocated to the Spark Streaming application. 

Therefore, it is important to remember that a Spark Streaming application needs to be allocated enough cores (or threads, if running locally) to process the received data, as well as to run the receiver(s).



Note **
When running a Spark Streaming program locally, do not use “local” or “local[1]” as the master URL. 
Either of these means that only one thread will be used for running tasks locally. 
If you are using an input DStream based on a receiver (e.g. sockets, Kafka, Flume, etc.), then the single thread will be used to run the receiver, leaving no thread for processing the received data. 
Hence, when running locally, always use “local[n]” as the master URL, where n > number of receivers to ru


Extending the logic to running on a cluster, the number of cores allocated to the Spark Streaming application must be more than the number of receivers. 
Otherwise the system will receive data, but not be able to process it.





-----------------------------------------------------------------------------------------------------------------------------------------------------

Streaming Sources

	Socket Source
		--  ssc.socketTextStream(...)
	
	File Stream
		For text file:
			streamingContext.textFileStream(dataDirectory)

		For reading data from files on any file system compatible with the HDFS API (that is, HDFS, S3, NFS, etc.), a DStream can be created as via 
		StreamingContext.fileStream[KeyClass, ValueClass, InputFormatClass]

		--  File streams do not require running a receiver so there is no need to allocate any cores for receiving file data.


	Queue of RDDs as a Stream
		For testing a Spark Streaming application with test data, one can also create a DStream based on a queue of RDDs, using streamingContext.queueStream(queueOfRDDs). Each RDD pushed into the queue will be treated as a batch of data in the DStream, and processed like a stream.

Advanced Sources
	Kafka
	Flume
	Kinesis

Note that these advanced sources are not available in the Spark shell, hence applications based on these advanced sources cannot be tested in the shell. If you really want to use them in the Spark shell you will have to download the corresponding Maven artifact’s JAR along with its dependencies and add it to the classpath.	


->  Custom Sources





Receiver Reliability

There can be two kinds of data sources based on their reliability. Sources (like Kafka and Flume) allow the transferred data to be acknowledged. 
If the system receiving data from these reliable sources acknowledges the received data correctly, it can be ensured that no data will be lost due to any kind of failure.

This leads to two kinds of receivers:
	Reliable Receiver - A reliable receiver correctly sends acknowledgment to a reliable source when the data has been received and stored in Spark with replication.
	
	Unreliable Receiver - An unreliable receiver does not send acknowledgment to a source. This can be used for sources that do not support acknowledgment, or even for reliable sources when one does not want or need to go into the complexity of acknowledgment.






-> Transformation on DStreams
	--  .transform() Operation

	--  Window Operation/Computations
		Apply transformations over sliding window of data




Join Operation
	Stream-stream joins:
		val stream1: DStream[String, String] = ...
		val stream2: DStream[String, String] = ...
		val joinedStream = stream1.join(stream2)

		val windowedStream1 = stream1.window(Seconds(20))
		val windowedStream2 = stream2.window(Minutes(1))
		val joinedStream = windowedStream1.join(windowedStream2)

	Stream-dataset joins
		val dataset: RDD[String, String] = ...
		val windowedStream = stream.window(Seconds(20))...
		val joinedStream = windowedStream.transform { rdd => rdd.join(dataset) }


Output Operations on DStreams
	print()
	saveAsTextFiles(prefix, [suffix]) 
	saveAsObjectFiles(prefix, [suffix])
	saveAsHadoopFiles(prefix, [suffix])
	foreachRDD(func) 






-> DataFrame and SQL Operations




-> MLlib Operations:
	You can also easily use machine learning algorithms provided by MLlib. 

	First of all, there are streaming machine learning algorithms (e.g. Streaming Linear Regression, Streaming KMeans, etc.) which can simultaneously learn from the streaming data as well as apply the model on the streaming data. 

	Beyond these, for a much larger class of machine learning algorithms, you can learn a learning model offline (i.e. using historical data) and then apply the model online on streaming data.






-> Caching / Persistenc
-> Checkpointing
-> Accumulators, Broadcast Variables, and Checkpoints