

Resources:
https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html

https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html
https://databricks.com/blog/2017/04/04/real-time-end-to-end-integration-with-apache-kafka-in-apache-sparks-structured-streaming.html
https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html
https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html

-----------------------------------------------------------------------------------------------------------------------------------------------------

Index:

Why Structured Streaming?
Spark Structured Streaming (Overview)
Streaming Programming Model
API using DataFrame / DataSets
Operations on streaming DataFrames/Datasets:
Window Operations on Event-Time
Handling Late Data and Watermarking
Join Operation
Output Mode
Output Sink Modes

Foreach
ForeachBatch
Trigger

Monitoring Streaming Queries
Reporting Metrics programmatically using Asynchronous APIs

Continuous Processing

-----------------------------------------------------------------------------------------------------------------------------------------------------

Why Structured Streaming?

Streaming Data Frameowrks -> Spark Streaming, Flink, Storm.
--  With the above streaming Frameowrks, developers stop worrying about issues related to Structured Application like - Fault-Tolerance, Zero Data Loss,
    Real-Time Processing etc, because above mentioned Frameowrks provide built-in support for all there issues.
--  By Checkpoinitng in Spark-Streaming and in Flink

--  Note: Structured Structured is not exactly same as Spark Streaming!!


Everything was fine with Streaming, but then along came the structured data era, where data is in Structured/Semi-Structured form. 
SQL on this type of data can only be executed once the data is ingested and is present in STATIC From.
-- SQL can only runs on Files, tables etc, and the Streaming World was totally untouched from it.


This compelled the big data industry experts to develop API(s) that can process streaming data present in structured/semi-structured form.

As a result, a lot of frameworks were developed that can process streaming data using SQL queries.
	Spark Structured Streaming
	KSQL (Kafka-SQL)
	Flink tables


Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine.

This means that we can express our streaming computation the same way we would express a batch computation on static data.


Since Structured Streaming is built over Spark SQL engine, it comes with a lot of advantages, like:
	--  Incremental and continous update of the final result (table) is taken care of by the API itself.
	--  Dataset/DataFrame API can be used/re-used to express streaming aggregations, event-time windows, stream-to-batch joins, etc.
	--  Computations are optimized, as the same Spark SQL engine is used.
	--  The application guarantees end-to-end exactly-once fault-tolerance through checkpointing and WALs (write-ahead logs).




-----------------------------------------------------------------------------------------------------------------------------------------------------

Spark Structured Streaming (Overview):

Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. [Why on SQL engine??]
You can express your streaming computation the same way you would express a batch computation on static data.
The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive.

You can use the Dataset/DataFrame API in Scala, Java, Python or R to express streaming aggregations, event-time windows, stream-to-batch joins, etc.
** Q. Does RDD's supported?

The computation is executed on the same optimized Spark SQL engine.
Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs.

Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming.


Internally -> Micro Bacthes (Data Stream as micro-batch jobs, with latency as low as 100 mili-seconds)

However, since Spark 2.3, we have introduced a new low-latency processing mode called Continuous Processing, which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees.


2 Model of Streaming:
	Micro Batch Processing Model (Default)
	Continuous Processing Model




Apache Spark 2.0 laid the foundational steps for a new higher-level API, Structured Streaming, for building continuous applications. 
[A streaming model that supports end-to-end applications that continuously react to data in real-time, We call them continuous applications that react to data in real-time.]
Apache Spark 2.1 extended support for data sources and data sinks, and buttressed streaming operations, including event-time processing watermarking, and checkpointing


Central to Structured Streaming is the notion that you treat a stream of data not as a stream but as an unbounded table. As new data arrives from the stream, new rows of DataFrames are appended to an unbounded table.

DStream (Discretized Stream) is the basic abstraction of Spark Streaming.




-----------------------------------------------------------------------------------------------------------------------------------------------------

Streaming Programming Model:

The key idea in Structured Streaming is to treat a live data stream as a table that is being continuously appended.
This leads to a new stream processing model that is very similar to a batch processing model. 
You will express your streaming computation as standard batch-like query AS ON A static table, and Spark runs it as an incremental query on the UNBOUNDED input table.
-- That is the input stream data can bee seen as it is appending to a input unbounded table, and we can write similar query as we would have written for a static table.

-> Data Stream as a Unbound Table

A query on the input will generate the “Result Table”. Every trigger interval (say, every 1 second), new rows get appended to the Input Table, which eventually updates the Result Table. 
Whenever the result table gets updated, we would want to write the changed result rows to an external sink.


The 'Output' is defined as what gets written out to the external storage. The output can be defined in a different mode:
	Complete Mode:
		The entire updated Result Table will be written to the external storage. 
		It is up to the storage connector to decide how to handle writing of the entire table.

	Append Mode:
		Only the new rows appended in the Result Table since the last trigger will be written to the external storage. 
		This is applicable only on the queries where existing rows in the Result Table are not expected to change.

	Update Mode:
		Only the rows that were updated in the Result Table since the last trigger will be written to the external storage (available since Spark 2.1.1). 
		Note that this is different from the Complete Mode in that this mode only outputs the rows that have changed since the last trigger. 
		If the query doesn’t contain aggregations, it will be equivalent to Append mode.



Note that Structured Streaming does not materialize the entire table. 
It reads the latest available data from the streaming data source, processes it incrementally to update the result, and then discards the source data. 
It only keeps around the minimal intermediate state data as required to update the result (e.g. intermediate counts in the earlier example).

This model is significantly different from many other stream processing engines. 
Many streaming systems require the user to maintain running aggregations themselves, thus having to reason about fault-tolerance, and data consistency (at-least-once, or at-most-once, or exactly-once). 
In this model, Spark is responsible for updating the Result Table when there is new data, thus relieving the users from reasoning about it.




-> Handling Event-time and Late Data (Water Marking - to allow users to specify the threshold on the late data)

-> Fault Tolerance Semantics (to delivering end to end exactly once semantic):
	-- Structured Streaming sources, the sinks and the execution engine to reliably track the exact progress of the processing so that it can handle any kind of failure by restarting and/or reprocessing.
	-- Every streaming source is assumed to have offsets (similar to Kafka offsets, or Kinesis sequence numbers) to track the read position in the stream.
	-- The engine uses checkpointing and write-ahead logs to record the offset range of the data being processed in each trigger.
	-- The streaming sinks are designed to be idempotent for handling reprocessing.




-----------------------------------------------------------------------------------------------------------------------------------------------------

API using DataFrame / DataSets

Spark 2.0 DataFrame and DataSets can represents static, bounded data, as well as streaming, unbounded data (Streaming DataFrame / DataSets).

Streaming DataFrames can be created through the DataStreamReader interface (Scala/Java/Python docs) returned by SparkSession.readStream()

Input Soruce:
	File Source (CSV, TXT, JSON, PARQUET, ORC)
	Kafka Source
	Socket Source
	Rate Source (for testing and benchmarking)



// Read text from socket
val socketDF = spark
  .readStream
  .format("socket")
  .option("host", "localhost")
  .option("port", 9999)
  .load()


socketDF.isStreaming    // Returns True for DataFrames that have streaming sources
socketDF.printSchema



// Read all the csv files written atomically in a directory
val userSchema = new StructType().add("name", "string").add("age", "integer")
val csvDF = spark
  .readStream
  .option("sep", ";")
  .schema(userSchema)            -- Specify schema of the csv files
  .csv("/path/to/directory")     -- Equivalent to format("csv").load("/path/to/directory")


These examples generate streaming DataFrames that are untyped, meaning that the schema of the DataFrame is not checked at compile time, only checked at runtime when the query is submitted. 
** Some operations like map, flatMap, etc. need the type to be known at compile time. 
To do those, you can convert these untyped streaming DataFrames to typed streaming Datasets using the same methods as static DataFrame.




--> Schema inference and partition of streaming DataFrames/Datasets
[https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#schema-inference-and-partition-of-streaming-dataframesdatasets]





Operations on streaming DataFrames/Datasets:
[https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#operations-on-streaming-dataframesdatasets]

You can apply all kinds of operations on streaming DataFrames/Datasets – ranging from untyped, SQL-like operations (e.g. select, where, groupBy), to typed RDD-like operations (e.g. map, filter, flatMap).


Example:

case class DeviceData(device: String, deviceType: String, signal: Double, time: DateTime)
val df: DataFrame = ...                            -- streaming DataFrame with IOT device data with schema { device: string, deviceType: string, signal: double, time: string }
val ds: Dataset[DeviceData] = df.as[DeviceData]    -- streaming Dataset with IOT device data

-- Select the devices which have signal more than 10
df.select("device").where("signal > 10")      -- using untyped APIs   
ds.filter(_.signal > 10).map(_.device)        -- using typed APIs

-- Running count of the number of updates for each device type, using untyped API
df.groupBy("deviceType").count()

-- Running average signal for each device type
import org.apache.spark.sql.expressions.scalalang.typed, using typed API
ds.groupByKey(_.deviceType).agg(typed.avg(_.signal))


-- Register the DataFrame/Dataset as temporaty view
df.createOrReplaceTempView("updates")
spark.sql("select count(*) from updates")  





-> Window Operations on Event-Time:
[https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#window-operations-on-event-time]
-- Note here that a single row can now fall into different time-event grouos too.


-> Handling Late Data and Watermarking
[https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking]
-- Late data after a certain time threshold, will get dropped.
-- .withWatermark("timestamp", "10 minutes")



-----------------------------------------------------------------------------------------------------------------------------------------------------

Join Operation:

Structured Streaming supports joining a streaming Dataset/DataFrame with a static Dataset/DataFrame as well as another streaming Dataset/DataFrame.
The result of the streaming join is generated incrementally, similar to the results of streaming aggregations in the previous section.
Note that in all the supported join types, the result of the join with a streaming Dataset/DataFrame will be the exactly the same as if it was with a static Dataset/DataFrame containing the same data in the stream.



Stream-StaticData Join:

	val staticDf = spark.read. ...
	val streamingDf = spark.readStream. ...

	streamingDf.join(staticDf, "type")                 -- inner equi-join with a static DF
	streamingDf.join(staticDf, "type", "right_join")   -- right outer join with a static DF



Stream-Stream Join:
	In Spark 2.3, we have added support for stream-stream joins, that is, you can join two streaming Datasets/DataFrames.










-----------------------------------------------------------------------------------------------------------------------------------------------------


Output Modes:
	Append Mode:
		This is the default mode, where only the new rows added to the Result Table since the last trigger will be outputted to the sink.

	Complete Mode:
		The whole Result Table will be outputted to the sink after every trigger. This is supported for aggregation queries.

	Update Mode:
		Only the rows in the Result Table that were updated since the last trigger will be outputted to the sink.


Output Sink:
	File Sink:
		writeStream.format("parquet/orc/csv/txt/json").option("path", "path/to/destination/dir").start()

	Kafka Sink:
		writeStream.format("kafka").option("kafka.bootstrap.servers", "host1:port1,host2:port2").option("topic", "updates").start()
	
	ForEach Sink:
		writeStream.foreach(...).start()

	Console Sink (for Debugging):
		-- Prints the output to the console/stdout every time there is a trigger.
		writeStream.format("console").start()

	Memory Sink:
		-- The output is stored in memory as an in-memory table.
		-- Used only with Low Data Volumes, as the entire volume is stored in Driver's Memory
		writeStream.format("memory").queryName("tableName").start()



-----------------------------------------------------------------------------------------------------------------------------------------------------

-- DF with no Aggregation
val noAggDF = deviceDataDf.select("device").where("signal > 10") 

-- write new data to console
noAggDF
  .writeStream
  .format("console")
  .start()


-- write new data to parquet files
.writeStream
  .format("parquet")
  .option("checkpointLocation", "path/to/checkpoint/dir")
  .option("path", "path/to/destination/dir")
  .start()



-- DF with Aggregation
val aggDF = df.groupBy("device").count()

-- Write updated aggregations to Console
aggDF
  .writeStream
  .outputMode("complete")
  .format("console")
  .start()


 -- Have all the aggregates in an in-memory table
aggDF
  .writeStream
  .queryName("aggregates")    -- this query name will be the table name
  .outputMode("complete")
  .format("memory")
  .start()



-- interactively query in-memory table
spark.sql("select * from aggregates").show()   





Using Foreach and ForeachBatch:

The foreach and foreachBatch operations allow you to apply arbitrary operations and writing logic on the output of a streaming query. 
They have slightly different use cases - while foreach allows custom write logic on every row, foreachBatch allows arbitrary operations and custom logic on the output of each micro-batch.


ForeachBatch:

streamingDF.writeStream.foreachBatch { (batchDF: DataFrame, batchId: Long) =>
  // Transform and write batchDF 
}.start()

Foreach:





-----------------------------------------------------------------------------------------------------------------------------------------------------

Triggers
The trigger settings of a streaming query defines the timing of streaming data processing, whether the query is going to executed as micro-batch query with a fixed batch interval or as a continuous processing query.


unspecified (default):
	If no trigger setting is explicitly specified, then by default, the query will be executed in micro-batch mode, where micro-batches will be generated as soon as the previous micro-batch has completed processing.

	-- Default trigger (runs micro-batch as soon as it can)
	df.writeStream.format("console").start()

Fixed interval micro-batches:
	df.writeStream.format("console").trigger(Trigger.ProcessingTime("2 seconds")).start()

One-time micro-batch:
	df.writeStream.format("console").trigger(Trigger.Once()).start()

Continuous with fixed checkpoint interval (Experimental):
	df.writeStream.format("console").trigger(Trigger.Continuous("1 second")).start()
	-- Continuous trigger with one-second checkpointing interval






Managing Streaming Queries:
The StreamingQuery object created when a query is started can be used to monitor and manage the query.

val query = df.writeStream.format("console").start()   // get the query object

query.id          // get the unique identifier of the running query that persists across restarts from checkpoint data

query.runId       // get the unique id of this run of the query, which will be generated at every start/restart

query.name        // get the name of the auto-generated or user-specified name

query.explain()   // print detailed explanations of the query

query.stop()      // stop the query

query.awaitTermination()   // block until query is terminated, with stop() or with error

query.exception       // the exception if the query has been terminated with error

query.recentProgress  // an array of the most recent progress updates for this query

query.lastProgress    // the most recent progress update of this streaming 



Note ***
You can start any number of queries in a single SparkSession. They will all be running concurrently sharing the cluster resources. 
You can use sparkSession.streams() to get the StreamingQueryManager (Scala/Java/Python docs) that can be used to manage the currently active queries.



val spark: SparkSession = ...

spark.streams.active    // get the list of currently active streaming queries

spark.streams.get(id)   // get a query object by its unique id

spark.streams.awaitAnyTermination()   // block until any one of them terminates





Monitoring Streaming Queries:
	streamingQuery.lastProgress()
	streamingQuery.status()
	streamingQuery.recentProgress

	val query: StreamingQuery = ...
	println(query.lastProgress





Reporting Metrics programmatically using Asynchronous APIs
[https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#reporting-metrics-programmatically-using-asynchronous-apis]

You can also asynchronously monitor all queries associated with a SparkSession by attaching a StreamingQueryListener.

Once you attach your custom StreamingQueryListener object with sparkSession.streams.attachListener(), you will get callbacks when a query is started and stopped and when there is progress made in an active query.


Example:

val spark: SparkSession = ...

spark.streams.addListener(new StreamingQueryListener() {
    override def onQueryStarted(queryStarted: QueryStartedEvent): Unit = {
        println("Query started: " + queryStarted.id)
    }
    override def onQueryTerminated(queryTerminated: QueryTerminatedEvent): Unit = {
        println("Query terminated: " + queryTerminated.id)
    }
    override def onQueryProgress(queryProgress: QueryProgressEvent): Unit = {
        println("Query made progress: " + queryProgress.progress)
    }
})



-> Reporting Metrics using Dropwizard Library 








Recovering from Failures with Checkpointing
In case of a failure or intentional shutdown, you can recover the previous progress and state of a previous query, and continue where it left off. 
This is done using checkpointing and write-ahead logs. 

You can configure a query with a checkpoint location, and the query will save all the progress information (i.e. range of offsets processed in each trigger) and the running aggregates (e.g. word counts in the quick example) to the checkpoint location. 
This checkpoint location has to be a path in an HDFS compatible file system, and can be set as an option in the DataStreamWriter when starting a query.

aggDF
  .writeStream
  .outputMode("complete")
  .option("checkpointLocation", "path/to/HDFS/dir")
  .format("memory")
  .start()





Continuous Processing:

Continuous processing is a new, experimental streaming execution mode introduced in Spark 2.3 that enables low (~1 ms) end-to-end latency with at-least-once fault-tolerance 
guarantees. 
Compare this with the default micro-batch processing engine which can achieve exactly-once guarantees but achieve latencies of ~100ms at best.

import org.apache.spark.sql.streaming.Trigger

spark
  .readStream
  .format("rate")
  .option("rowsPerSecond", "10")
  .option("")

spark
  .readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("subscribe", "topic1")
  .load()
  .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .writeStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("topic", "topic1")
  .trigger(Trigger.Continuous("1 second"))  // only change in query
  .start()